{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOGuPhnBcoYw09PDXaNIRef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5832fe768e024598839c48f582c46630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70ecda0d152d404193044587087a4c01",
              "IPY_MODEL_36768fd14de34cacaee3399dd8a79191"
            ],
            "layout": "IPY_MODEL_2764d641ad4442249e86e02ab2838cae"
          }
        },
        "70ecda0d152d404193044587087a4c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd9a801b5d6441f2824b0b4404c2c3c0",
            "placeholder": "​",
            "style": "IPY_MODEL_fa7282b908eb49749ca0050deee2df60",
            "value": "0.021 MB of 0.021 MB uploaded\r"
          }
        },
        "36768fd14de34cacaee3399dd8a79191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7c2035b95e444f3b22d49e590e2e554",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92905f4e2e93405b9bd7e5ee0b8775fa",
            "value": 1
          }
        },
        "2764d641ad4442249e86e02ab2838cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd9a801b5d6441f2824b0b4404c2c3c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa7282b908eb49749ca0050deee2df60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7c2035b95e444f3b22d49e590e2e554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92905f4e2e93405b9bd7e5ee0b8775fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a3d7c16782c4cf497cf79e3c9d9b223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eeca1277d61e4ad7acce9592255f2392",
              "IPY_MODEL_a748413390b0458a87b1e60121cc42ca"
            ],
            "layout": "IPY_MODEL_665d5a43613448bc87beba30cece4230"
          }
        },
        "eeca1277d61e4ad7acce9592255f2392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_173887c32400401faaebf51fed4bf9d8",
            "placeholder": "​",
            "style": "IPY_MODEL_b2490c5310ef41a8b135b7ce800ab512",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "a748413390b0458a87b1e60121cc42ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21453f13cbbb425db521012e07c3bba8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59583e9169244686adc523b1f2fb2842",
            "value": 1
          }
        },
        "665d5a43613448bc87beba30cece4230": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "173887c32400401faaebf51fed4bf9d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2490c5310ef41a8b135b7ce800ab512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21453f13cbbb425db521012e07c3bba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59583e9169244686adc523b1f2fb2842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakanKARASU/Awesome-Profile-README-templates/blob/master/Untitled33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMv-D__yV0pF",
        "outputId": "802c894f-2306-4c12-9628-bff5b9318f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Sep  2 07:28:02 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              52W / 400W |  12447MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bHUIq0TV9FO",
        "outputId": "927111c1-7904-4fe2-8b35-09bbc754ff18"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/intelligolabs/DIAG.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jCqedvKWAeM",
        "outputId": "ce372918-b11b-4244-f954-09c2e6c27377"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DIAG'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 54 (delta 11), reused 47 (delta 7), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (54/54), 1.70 MiB | 27.15 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1vS6m9gvvVBYw4ZSWuiIdqRn2dmgxWW-f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cmWpmYdWHR7",
        "outputId": "55f6ef84-fcc2-4af7-8d52-cea2c57a5a35"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vS6m9gvvVBYw4ZSWuiIdqRn2dmgxWW-f\n",
            "From (redirected): https://drive.google.com/uc?id=1vS6m9gvvVBYw4ZSWuiIdqRn2dmgxWW-f&confirm=t&uuid=069a51a9-a4d1-46dd-88fd-cbc00868ee68\n",
            "To: /content/DIAG/KolektorSDD2.zip\n",
            "100% 853M/853M [00:09<00:00, 91.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Name of the uploaded ZIP file\n",
        "zip_file = 'KolektorSDD2.zip'\n",
        "\n",
        "# Define the folder to extract the contents to\n",
        "extract_folder = '/content/KolektorSDD2'\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "# List the contents of the extracted folder to verify\n",
        "print(\"Extracted files:\")\n",
        "extracted_files = os.listdir(extract_folder)\n",
        "print(extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjbMlSYNWUej",
        "outputId": "e5fde602-1714-4b85-dcf1-e7b18fd0ee2a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files:\n",
            "['split_weakly_53.pyb', 'test', 'train', 'split_weakly_16.pyb', 'split_weakly_0.pyb', 'split_weakly_126.pyb', 'split_weakly_246.pyb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DIAG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iogF_KsXWlFT",
        "outputId": "b7496a1a-2b42-4a2f-f6a1-b1d6f0516812"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DIAG/DIAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ksdd2_preprocess.py --src_dir=\"/content/KolektorSDD2\" --dst_dir=\"/content/ksdd2_preprocessed_1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if5SXZ-9Wmf9",
        "outputId": "b01f9a91-b499-40d6-e3d0-b7234eb9933f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying files from /content/KolektorSDD2 to /content/ksdd2_preprocessed_1\n",
            "Copying .pyb files: 100% 5/5 [00:00<00:00, 441.86file/s]\n",
            "Reshaping train images: 100% 4664/4664 [00:35<00:00, 132.58file/s]\n",
            "Reshaping test images: 100% 2008/2008 [00:15<00:00, 133.53file/s]\n",
            "Creating train.csv: 100% 2332/2332 [00:59<00:00, 39.26file/s]\n",
            "Creating test.csv: 100% 1004/1004 [00:25<00:00, 39.24file/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBccdMzTpxP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_augmented_images.py --src_dir=\"/content/ksdd2_preprocessed_1\" --imgs_per_prompt=50  --seed=0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_rhNPQHXP8r",
        "outputId": "415d8a6e-4b08-46f9-e11c-5e517895e9c1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-09-02 07:33:29.685896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-02 07:33:29.706871: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-02 07:33:29.713233: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-02 07:33:30.907701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on device:  cuda\n",
            "Num negative images: 2085\n",
            "Num positive masks: 247\n",
            "Loading model diffusers/stable-diffusion-xl-1.0-inpainting-0.1\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:00,  8.28it/s]The config attributes {'decay': 0.9999, 'inv_gamma': 1.0, 'min_decay': 0.0, 'optimization_step': 37000, 'power': 0.6666666666666666, 'update_after_step': 0, 'use_ema_warmup': False} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  5.58it/s]\n",
            "Generating images for prompt: white marks on the wall\n",
            "100% 29/29 [00:03<00:00,  9.03it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.76it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.82it/s]\n",
            "100% 29/29 [00:03<00:00,  9.56it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.92it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.86it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.84it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.84it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.82it/s]\n",
            "100% 29/29 [00:02<00:00,  9.92it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "Generating images for prompt: copper metal scratches\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.83it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.86it/s]\n",
            "100% 29/29 [00:02<00:00,  9.91it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.78it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.90it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.85it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.86it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.89it/s]\n",
            "100% 29/29 [00:02<00:00,  9.87it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n",
            "100% 29/29 [00:02<00:00,  9.85it/s]\n",
            "100% 29/29 [00:02<00:00,  9.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import wandb\n",
        "import argparse\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from sklearn.metrics import average_precision_score, precision_score, recall_score, precision_recall_curve, roc_curve, auc\n",
        "import numpy as np\n",
        "\n",
        "from data.ksdd2 import KolektorSDD2\n"
      ],
      "metadata": {
        "id": "SJQTpZ7UrX_l"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, 1)\n",
        "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_pool = F.adaptive_avg_pool2d(x, 1)\n",
        "        avg_pool = self.fc2(F.relu(self.fc1(avg_pool)))\n",
        "        max_pool = F.adaptive_max_pool2d(x, 1)\n",
        "        max_pool = self.fc2(F.relu(self.fc1(max_pool)))\n",
        "        out = torch.sigmoid(avg_pool + max_pool)\n",
        "        return x * out\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(in_channels)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "nFHW1qwTMd6A"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class SimpleCNNWithCBAM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNNWithCBAM, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)  # Adjust input channels to 1 for grayscale\n",
        "        self.cbam = CBAM(64)  # CBAM for 64 channels\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 1)  # Adjust size as per your input dimensions\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.cbam(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64 * 16 * 16)  # Flatten the tensor\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "rtCCPZxyMf0r"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KSDD2ResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KSDD2ResNet50, self).__init__()\n",
        "\n",
        "        # Load the pre-trained ResNet-50 model from torchvision.models.\n",
        "        self.model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Change the output layer to output 1 class score instead of 1000 classes.\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "n5klksUpsTVj"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, criterion, test_loader, device, log_dict):\n",
        "    t_loss = 0\n",
        "    correct = 0\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, data in (tepoch := tqdm(enumerate(test_loader), unit='batch',\n",
        "                                       total=len(test_loader), desc='Validation')):\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # This gets the prediction from the network.\n",
        "            output = model(x)\n",
        "            output = output.squeeze(1)\n",
        "            # Sum up batch loss.\n",
        "            t_loss += criterion(output, y.float()).item()\n",
        "\n",
        "            # Get the prediction\n",
        "            pred = output\n",
        "\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "\n",
        "    t_loss /= len(test_loader)\n",
        "\n",
        "    precision_, recall_, thresholds = precision_recall_curve(targets, predictions)\n",
        "    f_measures = 2 * (precision_ * recall_) / (precision_ + recall_ + 0.0000000001)\n",
        "\n",
        "    # Select best threshold based on F2 score. Following previous works procedure.\n",
        "    ix_best = np.argmax(f_measures)\n",
        "    if ix_best > 0:\n",
        "        best_threshold = (thresholds[ix_best] + thresholds[ix_best - 1]) / 2\n",
        "    else:\n",
        "        best_threshold = thresholds[ix_best]\n",
        "    precision = precision_[ix_best]\n",
        "    recall = recall_[ix_best]\n",
        "\n",
        "    classifications = predictions > best_threshold\n",
        "\n",
        "    FPR, TPR, _ = roc_curve(targets, predictions)\n",
        "    AUC = auc(FPR, TPR)\n",
        "    AP = average_precision_score(targets, predictions)\n",
        "\n",
        "    # Calculate predictions based on best threshold.\n",
        "    correct = np.sum(classifications == targets)\n",
        "    accuracy = 100. * correct / len(classifications)\n",
        "\n",
        "    print('AVG loss: {:.4f}, ACC: {}/{} ({:.0f}%), Precision: {:.4f}, Recall: {:.4f}, AP: {:.4f}'.format(\n",
        "            t_loss, correct, len(test_loader.dataset), accuracy, precision, recall, AP))\n",
        "\n",
        "    # log metrics\n",
        "    log_dict['val_ACC'] = accuracy\n",
        "    log_dict['val_PRECISION'] = precision\n",
        "    log_dict['val_RECALL'] = recall\n",
        "    log_dict['val_AP'] = AP\n",
        "\n",
        "    return log_dict\n"
      ],
      "metadata": {
        "id": "UyglL3_OsWDK"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction, 1)\n",
        "        self.fc2 = nn.Conv2d(in_channels // reduction, in_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_pool = F.adaptive_avg_pool2d(x, 1)\n",
        "        avg_pool = self.fc2(F.relu(self.fc1(avg_pool)))\n",
        "        max_pool = F.adaptive_max_pool2d(x, 1)\n",
        "        max_pool = self.fc2(F.relu(self.fc1(max_pool)))\n",
        "        out = torch.sigmoid(avg_pool + max_pool)\n",
        "        return x * out\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(2, 1, 7, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = torch.cat([avg_pool, max_pool], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = ChannelAttention(in_channels)\n",
        "        self.spatial_attention = SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_attention(x)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "class SimpleCNNWithCBAM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNNWithCBAM, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)  # Adjust input channels to 3 for RGB\n",
        "        self.cbam = CBAM(64)  # CBAM for 64 channels\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 16 * 16, 1)  # Adjust size as per your input dimensions\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.cbam(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64 * 16 * 16)  # Flatten the tensor\n",
        "        x = self.fc1(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "5O5d8kQlNw2A"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming these are already initialized\n",
        "model = SimpleCNNWithCBAM().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "log_dict = {}\n",
        "\n",
        "# Run evaluation\n",
        "evaluate(model, criterion, test_loader, device, log_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "D6d97M3nscVD",
        "outputId": "131da2af-06bf-416f-c290-2a27b3b3d16a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/32 [00:00<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target size (torch.Size([32])) must be the same as input size (torch.Size([88]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-ca32a2ca36bd>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-89-c242747ec9d1>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, test_loader, device, log_dict)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Sum up batch loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mt_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    735\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([32])) must be the same as input size (torch.Size([88]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    # Set the seed for reproducibility.\n",
        "    torch.manual_seed(args.seed)\n",
        "    # Set the device.\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    add_augmented = args.add_augmented\n",
        "    num_augmented = args.num_augmented\n",
        "    zero_shot = args.zero_shot\n",
        "    logging = args.logging\n",
        "\n",
        "    run_name = f'KSDD2ResNet50-zero_shot_{zero_shot}-add_augmented_{add_augmented}-num_augmented_{num_augmented}-bs_{args.batch_size}-epochs_{args.epochs}'\n",
        "    tags = [f'{args.epochs}epochs', f'{num_augmented}augmented']\n",
        "    if args.zero_shot:\n",
        "        tags.append('zero_shot')\n",
        "    else:\n",
        "        tags.append('full_shot')\n",
        "    if args.add_augmented:\n",
        "        tags.append('augmented')\n",
        "    else:\n",
        "        tags.append('not_augmented')\n",
        "\n",
        "    if logging:\n",
        "        # Start a new wandb run to track this script.\n",
        "        wandb.init(\n",
        "            name=run_name,\n",
        "            config=args,\n",
        "            tags=tags\n",
        "        )\n",
        "\n",
        "    # Dataset.\n",
        "    print('Loading KolektorSDD2 training set...')\n",
        "    train_data = KolektorSDD2(dataroot=args.dataset_path, split='train', add_augmented=add_augmented, num_augmented=num_augmented, zero_shot=zero_shot)\n",
        "    print('Number of samples:', len(train_data))\n",
        "\n",
        "    print('Loading KolektorSDD2 test set...')\n",
        "    test_data = KolektorSDD2(dataroot=args.dataset_path, split='test')\n",
        "    print('Number of samples:', len(test_data))\n",
        "\n",
        "    # DataLoaders.\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size,\n",
        "                              shuffle=True, num_workers=args.num_workers)\n",
        "    test_loader = DataLoader(test_data, batch_size=args.batch_size,\n",
        "                             shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    # Define the model.\n",
        "    model = KSDD2ResNet50()\n",
        "    model.to(device)\n",
        "\n",
        "    # Define the loss function and the optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training step.\n",
        "    print(f'Start training on {device} [...]')\n",
        "    model.train()\n",
        "    log_dict = {'train_loss': 0, 'val_ACC': 0, 'val_PRECISION': 0, 'val_RECALL': 0, 'val_AP': 0, 'epoch': 0}\n",
        "    for e in range(args.epochs):\n",
        "        epoch_loss = 0\n",
        "        for _, data in (tepoch := tqdm(enumerate(train_loader), unit='batch',\n",
        "                                       total=len(train_loader))):\n",
        "            tepoch.set_description(f'Epoch {e}')\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Training step for the single batch.\n",
        "            model.zero_grad()\n",
        "            outputs = model(x)\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = criterion(outputs, y.float())\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics.\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "            if logging:\n",
        "                wandb.log({'train_loss':loss.item()})\n",
        "        epoch_loss /= len(train_loader)\n",
        "        log_dict['epoch_loss'] = epoch_loss\n",
        "        log_dict['epoch'] = e\n",
        "\n",
        "        # Evaluation step after each epoch.\n",
        "        eval_dict = evaluate(model, criterion, test_loader, device, log_dict)\n",
        "        if logging:\n",
        "            wandb.log(eval_dict)\n",
        "\n",
        "    if logging:\n",
        "        wandb.finish()\n",
        "    print('Training finished.')\n"
      ],
      "metadata": {
        "id": "XzWj6DW1sqFd"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    t_loss = 0\n",
        "    correct = 0\n",
        "    predictions = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, unit='batch', desc='Test'):\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(x)\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = criterion(outputs, y.float())\n",
        "            t_loss += loss.item()\n",
        "\n",
        "            # Collect predictions and targets for metrics calculation\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "\n",
        "    t_loss /= len(test_loader)\n",
        "\n",
        "    # Compute metrics\n",
        "    predictions = np.array(predictions)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(targets, predictions)\n",
        "    f_measures = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "    ix_best = np.argmax(f_measures)\n",
        "    best_threshold = (thresholds[ix_best] + thresholds[ix_best - 1]) / 2 if ix_best > 0 else thresholds[ix_best]\n",
        "    precision = precision[ix_best]\n",
        "    recall = recall[ix_best]\n",
        "\n",
        "    classifications = predictions > best_threshold\n",
        "    accuracy = 100. * np.sum(classifications == targets) / len(targets)\n",
        "\n",
        "    # Calculate AUC and AP\n",
        "    FPR, TPR, _ = roc_curve(targets, predictions)\n",
        "    AUC = auc(FPR, TPR)\n",
        "    AP = average_precision_score(targets, predictions)\n",
        "\n",
        "    # Print test results\n",
        "    print(f'Test Loss: {t_loss:.4f}, Accuracy: {accuracy:.2f}%, Precision: {precision:.4f}, Recall: {recall:.4f}, AP: {AP:.4f}')\n",
        "\n",
        "    return {\n",
        "        'test_loss': t_loss,\n",
        "        'test_accuracy': accuracy,\n",
        "        'test_precision': precision,\n",
        "        'test_recall': recall,\n",
        "        'test_ap': AP\n",
        "    }"
      ],
      "metadata": {
        "id": "NgEB6OEQtEs2"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    # Set the seed for reproducibility.\n",
        "    torch.manual_seed(args.seed)\n",
        "    # Set the device.\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    add_augmented = args.add_augmented\n",
        "    num_augmented = args.num_augmented\n",
        "    zero_shot = args.zero_shot\n",
        "    logging = args.logging\n",
        "\n",
        "    run_name = f'KSDD2ResNet50-zero_shot_{zero_shot}-add_augmented_{add_augmented}-num_augmented_{num_augmented}-bs_{args.batch_size}-epochs_{args.epochs}'\n",
        "    tags = [f'{args.epochs}epochs', f'{num_augmented}augmented']\n",
        "    if args.zero_shot:\n",
        "        tags.append('zero_shot')\n",
        "    else:\n",
        "        tags.append('full_shot')\n",
        "    if args.add_augmented:\n",
        "        tags.append('augmented')\n",
        "    else:\n",
        "        tags.append('not_augmented')\n",
        "\n",
        "    if logging:\n",
        "        # Start a new wandb run to track this script.\n",
        "        wandb.init(\n",
        "            name=run_name,\n",
        "            config=args,\n",
        "            tags=tags\n",
        "        )\n",
        "\n",
        "    # Dataset.\n",
        "    print('Loading KolektorSDD2 training set...')\n",
        "    train_data = KolektorSDD2(dataroot=args.dataset_path, split='train', add_augmented=add_augmented, num_augmented=num_augmented, zero_shot=zero_shot)\n",
        "    print('Number of samples:', len(train_data))\n",
        "\n",
        "    print('Loading KolektorSDD2 test set...')\n",
        "    test_data = KolektorSDD2(dataroot=args.dataset_path, split='test')\n",
        "    print('Number of samples:', len(test_data))\n",
        "\n",
        "    # DataLoaders.\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size,\n",
        "                              shuffle=True, num_workers=args.num_workers)\n",
        "    test_loader = DataLoader(test_data, batch_size=args.batch_size,\n",
        "                             shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    # Define the model.\n",
        "    model = KSDD2ResNet50()\n",
        "    model.to(device)\n",
        "\n",
        "    # Define the loss function and the optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training step.\n",
        "    print(f'Start training on {device} [...]')\n",
        "    model.train()\n",
        "    log_dict = {'train_loss': 0, 'val_ACC': 0, 'val_PRECISION': 0, 'val_RECALL': 0, 'val_AP': 0, 'epoch': 0}\n",
        "    for e in range(args.epochs):\n",
        "        epoch_loss = 0\n",
        "        for _, data in (tepoch := tqdm(enumerate(train_loader), unit='batch',\n",
        "                                       total=len(train_loader))):\n",
        "            tepoch.set_description(f'Epoch {e}')\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Training step for the single batch.\n",
        "            model.zero_grad()\n",
        "            outputs = model(x)\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = criterion(outputs, y.float())\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics.\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "            if logging:\n",
        "                wandb.log({'train_loss':loss.item()})\n",
        "        epoch_loss /= len(train_loader)\n",
        "        log_dict['epoch_loss'] = epoch_loss\n",
        "        log_dict['epoch'] = e\n",
        "\n",
        "        # Evaluation step after each epoch.\n",
        "        eval_dict = evaluate(model, criterion, test_loader, device, log_dict)\n",
        "        if logging:\n",
        "            wandb.log(eval_dict)\n",
        "\n",
        "    # Test the model after training\n",
        "    test_dict = test(model, criterion, test_loader, device)\n",
        "    if logging:\n",
        "        wandb.log(test_dict)\n",
        "\n",
        "    if logging:\n",
        "        wandb.finish()\n",
        "    print('Training finished.')\n"
      ],
      "metadata": {
        "id": "4k3H6C9NG836"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.seed = 1234\n",
        "        self.epochs = 30\n",
        "        self.batch_size = 32\n",
        "        self.num_workers = 2\n",
        "        self.dataset_path = 'ksdd2_preprocessed_1'\n",
        "        self.add_augmented = True\n",
        "        self.num_augmented = 100\n",
        "        self.zero_shot = False\n",
        "        self.logging = True\n",
        "\n",
        "args = Args()\n"
      ],
      "metadata": {
        "id": "g3dLJt5FsvfU"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5832fe768e024598839c48f582c46630",
            "70ecda0d152d404193044587087a4c01",
            "36768fd14de34cacaee3399dd8a79191",
            "2764d641ad4442249e86e02ab2838cae",
            "cd9a801b5d6441f2824b0b4404c2c3c0",
            "fa7282b908eb49749ca0050deee2df60",
            "a7c2035b95e444f3b22d49e590e2e554",
            "92905f4e2e93405b9bd7e5ee0b8775fa"
          ]
        },
        "id": "PYJPMdN1tFkk",
        "outputId": "b5fd3f06-9da9-4e16-ba86-e192dc59ce0f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240902_094108-ga2508q1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hakankarasu97-kist/uncategorized/runs/ga2508q1' target=\"_blank\">KSDD2ResNet50-zero_shot_False-add_augmented_True-num_augmented_100-bs_32-epochs_30</a></strong> to <a href='https://wandb.ai/hakankarasu97-kist/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hakankarasu97-kist/uncategorized' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hakankarasu97-kist/uncategorized/runs/ga2508q1' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/uncategorized/runs/ga2508q1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading KolektorSDD2 training set...\n",
            "Number of samples: 2431\n",
            "Loading KolektorSDD2 test set...\n",
            "Number of samples: 1004\n",
            "Start training on cuda [...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 76/76 [00:13<00:00,  5.70batch/s, loss=0.0493]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.59batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1117, ACC: 980/1004 (98%), Precision: 0.9300, Recall: 0.8455, AP: 0.9076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 1: 100%|██████████| 76/76 [00:12<00:00,  5.85batch/s, loss=0.068]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.57batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1615, ACC: 970/1004 (97%), Precision: 0.9043, Recall: 0.7727, AP: 0.8183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2: 100%|██████████| 76/76 [00:12<00:00,  5.86batch/s, loss=0.146]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.35batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1566, ACC: 971/1004 (97%), Precision: 0.9425, Recall: 0.7455, AP: 0.8308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3: 100%|██████████| 76/76 [00:12<00:00,  5.87batch/s, loss=0.141]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.54batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1529, ACC: 966/1004 (96%), Precision: 0.9390, Recall: 0.7000, AP: 0.7952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4: 100%|██████████| 76/76 [00:12<00:00,  5.85batch/s, loss=0.145]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.48batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.0894, ACC: 978/1004 (97%), Precision: 0.8889, Recall: 0.8727, AP: 0.9236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5: 100%|██████████| 76/76 [00:12<00:00,  5.85batch/s, loss=0.0396]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.49batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1389, ACC: 972/1004 (97%), Precision: 0.8750, Recall: 0.8273, AP: 0.9066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6: 100%|██████████| 76/76 [00:13<00:00,  5.82batch/s, loss=0.0207]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.51batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.0928, ACC: 977/1004 (97%), Precision: 0.9882, Recall: 0.7636, AP: 0.9138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7: 100%|██████████| 76/76 [00:12<00:00,  5.87batch/s, loss=0.019]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.41batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1917, ACC: 971/1004 (97%), Precision: 0.8532, Recall: 0.8455, AP: 0.8979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8: 100%|██████████| 76/76 [00:12<00:00,  5.85batch/s, loss=0.0922]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.52batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1051, ACC: 974/1004 (97%), Precision: 0.9000, Recall: 0.8182, AP: 0.9225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9: 100%|██████████| 76/76 [00:13<00:00,  5.78batch/s, loss=0.123]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.43batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1381, ACC: 971/1004 (97%), Precision: 0.8969, Recall: 0.7909, AP: 0.8468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10: 100%|██████████| 76/76 [00:13<00:00,  5.83batch/s, loss=0.177]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.18batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1359, ACC: 978/1004 (97%), Precision: 0.9286, Recall: 0.8273, AP: 0.8800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11: 100%|██████████| 76/76 [00:13<00:00,  5.82batch/s, loss=0.133]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.46batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1194, ACC: 978/1004 (97%), Precision: 0.9565, Recall: 0.8000, AP: 0.9182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12: 100%|██████████| 76/76 [00:12<00:00,  5.86batch/s, loss=0.146]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.47batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1461, ACC: 970/1004 (97%), Precision: 0.8800, Recall: 0.8000, AP: 0.8735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13: 100%|██████████| 76/76 [00:13<00:00,  5.82batch/s, loss=0.00454]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.41batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1157, ACC: 980/1004 (98%), Precision: 0.9300, Recall: 0.8455, AP: 0.9019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14: 100%|██████████| 76/76 [00:13<00:00,  5.84batch/s, loss=0.175]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.31batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1214, ACC: 981/1004 (98%), Precision: 0.9394, Recall: 0.8455, AP: 0.9131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 15: 100%|██████████| 76/76 [00:13<00:00,  5.84batch/s, loss=0.0153]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.33batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1971, ACC: 975/1004 (97%), Precision: 0.9263, Recall: 0.8000, AP: 0.9170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16: 100%|██████████| 76/76 [00:13<00:00,  5.83batch/s, loss=0.0231]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.47batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1297, ACC: 969/1004 (97%), Precision: 0.8440, Recall: 0.8364, AP: 0.8803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 17: 100%|██████████| 76/76 [00:13<00:00,  5.84batch/s, loss=0.00423]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.49batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1371, ACC: 972/1004 (97%), Precision: 0.8679, Recall: 0.8364, AP: 0.9181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 18: 100%|██████████| 76/76 [00:13<00:00,  5.84batch/s, loss=0.00155]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.41batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1823, ACC: 969/1004 (97%), Precision: 0.8319, Recall: 0.8545, AP: 0.8931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 19: 100%|██████████| 76/76 [00:12<00:00,  5.85batch/s, loss=0.00943]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.47batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1572, ACC: 980/1004 (98%), Precision: 0.9574, Recall: 0.8182, AP: 0.9152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 20: 100%|██████████| 76/76 [00:13<00:00,  5.83batch/s, loss=0.0905]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.41batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1191, ACC: 974/1004 (97%), Precision: 0.8922, Recall: 0.8273, AP: 0.8809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 21: 100%|██████████| 76/76 [00:13<00:00,  5.81batch/s, loss=0.00598]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.29batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1873, ACC: 973/1004 (97%), Precision: 0.8692, Recall: 0.8455, AP: 0.9167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 22: 100%|██████████| 76/76 [00:13<00:00,  5.83batch/s, loss=0.00345]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.45batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1768, ACC: 973/1004 (97%), Precision: 0.8559, Recall: 0.8636, AP: 0.9159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 23: 100%|██████████| 76/76 [00:13<00:00,  5.82batch/s, loss=0.00398]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.40batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1394, ACC: 974/1004 (97%), Precision: 0.8704, Recall: 0.8545, AP: 0.9079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 24: 100%|██████████| 76/76 [00:13<00:00,  5.84batch/s, loss=0.00286]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.39batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1726, ACC: 969/1004 (97%), Precision: 0.8205, Recall: 0.8727, AP: 0.8845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 25: 100%|██████████| 76/76 [00:12<00:00,  5.85batch/s, loss=0.0023]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.42batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.2086, ACC: 970/1004 (97%), Precision: 0.8878, Recall: 0.7909, AP: 0.8360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 26: 100%|██████████| 76/76 [00:13<00:00,  5.80batch/s, loss=0.00151]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.25batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.2023, ACC: 972/1004 (97%), Precision: 0.8900, Recall: 0.8091, AP: 0.9054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 27: 100%|██████████| 76/76 [00:13<00:00,  5.81batch/s, loss=0.00448]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.46batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1998, ACC: 972/1004 (97%), Precision: 0.9643, Recall: 0.7364, AP: 0.8732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 28: 100%|██████████| 76/76 [00:13<00:00,  5.82batch/s, loss=0.000593]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.26batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1975, ACC: 970/1004 (97%), Precision: 0.8654, Recall: 0.8182, AP: 0.9011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 29: 100%|██████████| 76/76 [00:13<00:00,  5.82batch/s, loss=0.00189]\n",
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 11.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVG loss: 0.1685, ACC: 974/1004 (97%), Precision: 0.9348, Recall: 0.7818, AP: 0.8966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 32/32 [00:03<00:00,  9.86batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1685, Accuracy: 97.01%, Precision: 0.9348, Recall: 0.7818, AP: 0.8966\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5832fe768e024598839c48f582c46630"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch_loss</td><td>█▆█▆▅▄▃▄▃▃▃▂▂▂▁▃▃▂▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_ap</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_loss</td><td>▃▄▁█▄▁▃▂▂▁▄▁▆▁▅▁▁▁▁▂▁▁▁▆▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_ACC</td><td>█▃▃▁▇▄▆▃▅▃▇▇▃██▅▂▄▂█▅▄▄▅▂▃▄▄▃▅</td></tr><tr><td>val_AP</td><td>▇▂▃▁█▇▇▇█▄▆█▅▇▇█▆█▆█▆██▇▆▃▇▅▇▇</td></tr><tr><td>val_PRECISION</td><td>▆▄▆▆▄▃█▂▄▄▆▇▃▆▆▅▂▃▁▇▄▃▂▃▁▄▄▇▃▆</td></tr><tr><td>val_RECALL</td><td>▇▄▃▁█▆▄▇▆▅▆▅▅▇▇▅▇▇▇▆▆▇█▇█▅▅▂▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>epoch_loss</td><td>0.00705</td></tr><tr><td>test_accuracy</td><td>97.01195</td></tr><tr><td>test_ap</td><td>0.89656</td></tr><tr><td>test_loss</td><td>0.16847</td></tr><tr><td>test_precision</td><td>0.93478</td></tr><tr><td>test_recall</td><td>0.78182</td></tr><tr><td>train_loss</td><td>0</td></tr><tr><td>val_ACC</td><td>97.01195</td></tr><tr><td>val_AP</td><td>0.89656</td></tr><tr><td>val_PRECISION</td><td>0.93478</td></tr><tr><td>val_RECALL</td><td>0.78182</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">KSDD2ResNet50-zero_shot_False-add_augmented_True-num_augmented_100-bs_32-epochs_30</strong> at: <a href='https://wandb.ai/hakankarasu97-kist/uncategorized/runs/ga2508q1' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/uncategorized/runs/ga2508q1</a><br/> View project at: <a href='https://wandb.ai/hakankarasu97-kist/uncategorized' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/uncategorized</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240902_094108-ga2508q1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S71GsAPRt7Bu",
        "outputId": "32dbd3c1-d936-44ca-b946-c6d9ef972094"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "class KSDD2ResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KSDD2ResNet50, self).__init__()\n",
        "\n",
        "        # Load the pre-trained ResNet-50 model from torchvision.models.\n",
        "        self.model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Change the output layer to output 1 class score instead of 1000 classes.\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "2IfUPc_SZYRb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_curve, auc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(model, criterion, test_loader, device, log_dict):\n",
        "    t_loss = 0\n",
        "    correct = 0\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, data in (tepoch := tqdm(enumerate(test_loader), unit='batch',\n",
        "                                       total=len(test_loader), desc='Validation')):\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # This gets the prediction from the network.\n",
        "            output = model(x)\n",
        "            output = output.squeeze(1)\n",
        "            # Sum up batch loss.\n",
        "            t_loss += criterion(output, y.float()).item()\n",
        "\n",
        "            # Get the prediction\n",
        "            pred = output\n",
        "\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "\n",
        "    t_loss /= len(test_loader)\n",
        "\n",
        "    precision_, recall_, thresholds = precision_recall_curve(targets, predictions)\n",
        "    f_measures = 2 * (precision_ * recall_) / (precision_ + recall_ + 0.0000000001)\n",
        "\n",
        "    # Select best threshold based on F2 score.\n",
        "    ix_best = np.argmax(f_measures)\n",
        "    if ix_best > 0:\n",
        "        best_threshold = (thresholds[ix_best] + thresholds[ix_best - 1]) / 2\n",
        "    else:\n",
        "        best_threshold = thresholds[ix_best]\n",
        "    precision = precision_[ix_best]\n",
        "    recall = recall_[ix_best]\n",
        "\n",
        "    classifications = np.array(predictions) > best_threshold\n",
        "\n",
        "    FPR, TPR, _ = roc_curve(targets, predictions)\n",
        "    AUC = auc(FPR, TPR)\n",
        "    AP = average_precision_score(targets, predictions)\n",
        "\n",
        "    # Calculate predictions based on best threshold.\n",
        "    correct = np.sum(classifications == np.array(targets))\n",
        "    accuracy = 100. * correct / len(classifications)\n",
        "\n",
        "    print('AVG loss: {:.4f}, ACC: {}/{} ({:.0f}%), Precision: {:.4f}, Recall: {:.4f}, AP: {:.4f}'.format(\n",
        "            t_loss, correct, len(test_loader.dataset), accuracy, precision, recall, AP))\n",
        "\n",
        "    # log metrics\n",
        "    log_dict['val_ACC'] = accuracy\n",
        "    log_dict['val_PRECISION'] = precision\n",
        "    log_dict['val_RECALL'] = recall\n",
        "    log_dict['val_AP'] = AP\n",
        "\n",
        "    return log_dict\n"
      ],
      "metadata": {
        "id": "7s_VmOvnnTzk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.randn(3, 224, 224)  # Dummy image tensor\n",
        "        y = torch.randint(0, 2, (1,)).float()  # Dummy label\n",
        "        return x, y\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(DummyDataset(10), batch_size=2)\n",
        "test_loader = DataLoader(DummyDataset(5), batch_size=2)\n",
        "\n",
        "print(\"Dummy DataLoader and Dataset defined.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06qcPxtRnVPc",
        "outputId": "0ee7d89e-cebd-438c-d693-1a94e892e8c9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy DataLoader and Dataset defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    # Set the seed for reproducibility.\n",
        "    torch.manual_seed(args.seed)\n",
        "    # Set the device.\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    add_augmented = args.add_augmented\n",
        "    num_augmented = args.num_augmented\n",
        "    zero_shot = args.zero_shot\n",
        "    logging = args.logging\n",
        "\n",
        "    run_name = f'KSDD2ResNet50-zero_shot_{zero_shot}-add_augmented_{add_augmented}-num_augmented_{num_augmented}-bs_{args.batch_size}-epochs_{args.epochs}'\n",
        "    tags = [f'{args.epochs}epochs', f'{num_augmented}augmented']\n",
        "    if args.zero_shot:\n",
        "        tags.append('zero_shot')\n",
        "    else:\n",
        "        tags.append('full_shot')\n",
        "    if args.add_augmented:\n",
        "        tags.append('augmented')\n",
        "    else:\n",
        "        tags.append('not_augmented')\n",
        "\n",
        "    if logging:\n",
        "        # Start a new wandb run to track this script.\n",
        "        wandb.init(\n",
        "            name=run_name,\n",
        "            config=args,\n",
        "            tags=tags\n",
        "        )\n",
        "\n",
        "    # Define the model.\n",
        "    model = KSDD2ResNet50()\n",
        "    model.to(device)\n",
        "\n",
        "    # Define the loss function and the optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training step.\n",
        "    print(f'Start training on {device} [...]')\n",
        "    model.train()\n",
        "    log_dict = {'train_loss': 0, 'val_ACC': 0, 'val_PRECISION': 0, 'val_RECALL': 0, 'val_AP': 0, 'epoch': 0}\n",
        "    for e in range(args.epochs):\n",
        "        epoch_loss = 0\n",
        "        for _, data in (tepoch := tqdm(enumerate(train_loader), unit='batch',\n",
        "                                       total=len(train_loader))):\n",
        "            tepoch.set_description(f'Epoch {e}')\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Training step for the single batch.\n",
        "            model.zero_grad()\n",
        "            outputs = model(x)\n",
        "            outputs = outputs.squeeze(1)  # Ensure outputs are of shape [batch_size]\n",
        "            y = y.squeeze(1)  # Ensure targets are of shape [batch_size]\n",
        "            loss = criterion(outputs, y.float())\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics.\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "            if logging:\n",
        "                wandb.log({'train_loss':loss.item()})\n",
        "        epoch_loss /= len(train_loader)\n",
        "        log_dict['epoch_loss'] = epoch_loss\n",
        "        log_dict['epoch'] = e\n",
        "\n",
        "        # Evaluation step after each epoch.\n",
        "        eval_dict = evaluate(model, criterion, test_loader, device, log_dict)\n",
        "        if logging:\n",
        "            wandb.log(eval_dict)\n",
        "\n",
        "    if logging:\n",
        "        wandb.finish()\n",
        "    print('Training finished.')\n"
      ],
      "metadata": {
        "id": "eLau7Hd3nZMN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Create a dummy argument parser for testing\n",
        "    class Args:\n",
        "        seed = 1234\n",
        "        epochs = 1  # Set to 1 for quicker testing\n",
        "        batch_size = 2\n",
        "        num_workers = 1\n",
        "        dataset_path = \"/content/ksdd2_preprocessed_1\"\n",
        "        add_augmented = True\n",
        "        num_augmented = 100\n",
        "        zero_shot = True\n",
        "        logging = False\n",
        "\n",
        "    args = Args()\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "CdIimCu8n3br",
        "outputId": "802503b0-63b7-4d39-9e73-011c4d19d04b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training on cuda [...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 5/5 [00:00<00:00, 20.83batch/s, loss=0.604]\n",
            "Validation:   0%|          | 0/3 [00:00<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target size (torch.Size([2, 1])) must be the same as input size (torch.Size([2]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-2b2daa09ed87>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-25deb71e968b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Evaluation step after each epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0meval_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-5d94184b9b29>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, test_loader, device, log_dict)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Sum up batch loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mt_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    735\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([2, 1])) must be the same as input size (torch.Size([2]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_curve, auc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate(model, criterion, test_loader, device, log_dict):\n",
        "    t_loss = 0\n",
        "    correct = 0\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, data in (tepoch := tqdm(enumerate(test_loader), unit='batch',\n",
        "                                       total=len(test_loader), desc='Validation')):\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # This gets the prediction from the network.\n",
        "            output = model(x)\n",
        "            output = output.squeeze(1)\n",
        "            # Sum up batch loss.\n",
        "            t_loss += criterion(output, y.float()).item()\n",
        "\n",
        "            # Get the prediction\n",
        "            pred = output\n",
        "\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "\n",
        "    t_loss /= len(test_loader)\n",
        "\n",
        "    precision_, recall_, thresholds = precision_recall_curve(targets, predictions)\n",
        "    f_measures = 2 * (precision_ * recall_) / (precision_ + recall_ + 0.0000000001)\n",
        "\n",
        "    # Select best threshold based on F2 score.\n",
        "    ix_best = np.argmax(f_measures)\n",
        "    if ix_best > 0:\n",
        "        best_threshold = (thresholds[ix_best] + thresholds[ix_best - 1]) / 2\n",
        "    else:\n",
        "        best_threshold = thresholds[ix_best]\n",
        "    precision = precision_[ix_best]\n",
        "    recall = recall_[ix_best]\n",
        "\n",
        "    classifications = np.array(predictions) > best_threshold\n",
        "\n",
        "    FPR, TPR, _ = roc_curve(targets, predictions)\n",
        "    AUC = auc(FPR, TPR)\n",
        "    AP = average_precision_score(targets, predictions)\n",
        "\n",
        "    # Calculate predictions based on best threshold.\n",
        "    correct = np.sum(classifications == np.array(targets))\n",
        "    accuracy = 100. * correct / len(classifications)\n",
        "\n",
        "    print('AVG loss: {:.4f}, ACC: {}/{} ({:.0f}%), Precision: {:.4f}, Recall: {:.4f}, AP: {:.4f}'.format(\n",
        "            t_loss, correct, len(test_loader.dataset), accuracy, precision, recall, AP))\n",
        "\n",
        "    # log metrics\n",
        "    log_dict['val_ACC'] = accuracy\n",
        "    log_dict['val_PRECISION'] = precision\n",
        "    log_dict['val_RECALL'] = recall\n",
        "    log_dict['val_AP'] = AP\n",
        "\n",
        "    return log_dict\n"
      ],
      "metadata": {
        "id": "YR40Jv-CmZwE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model\n",
        "model = KSDD2ResNet50()\n",
        "model.to(device)\n",
        "\n",
        "# Print model architecture\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkPSAvIBmcgD",
        "outputId": "bdb9c7b5-9834-491b-c5d8-451cf69132dc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KSDD2ResNet50(\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.randn(3, 224, 224)  # Dummy image tensor\n",
        "        y = torch.randint(0, 2, (1,)).float()  # Dummy label\n",
        "        return x, y\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(DummyDataset(10), batch_size=2)\n",
        "test_loader = DataLoader(DummyDataset(5), batch_size=2)\n"
      ],
      "metadata": {
        "id": "Jov3adM4mh7E"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"Criterion and Optimizer initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_Wk8woEmowc",
        "outputId": "8470dbdc-36af-405d-b190-36681d574010"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Criterion and Optimizer initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the evaluation function\n",
        "log_dict = {'val_ACC': 0, 'val_PRECISION': 0, 'val_RECALL': 0, 'val_AP': 0}\n",
        "log_dict = evaluate(model, criterion, test_loader, device, log_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vomJVpBfmt2F",
        "outputId": "ead7a1aa-3956-47f8-a811-9147db886de8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/3 [00:00<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target size (torch.Size([2, 1])) must be the same as input size (torch.Size([2]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-c0c3734934a8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the evaluation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlog_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_ACC'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_PRECISION'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_RECALL'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_AP'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlog_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-5d94184b9b29>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, criterion, test_loader, device, log_dict)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Sum up batch loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mt_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    735\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([2, 1])) must be the same as input size (torch.Size([2]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KSDD2ResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KSDD2ResNet50, self).__init__()\n",
        "\n",
        "        # Load the pre-trained ResNet-50 model from torchvision.models.\n",
        "        self.model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Change the output layer to output 1 class score instead of 1000 classes.\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "# Instantiate and print the model to verify\n",
        "model = KSDD2ResNet50()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjAGUkzYlmn0",
        "outputId": "152697b5-fc83-448a-9455-17918863a761"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KSDD2ResNet50(\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, criterion, test_loader, device, log_dict):\n",
        "    t_loss = 0\n",
        "    correct = 0\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, data in (tepoch := tqdm(enumerate(test_loader), unit='batch',\n",
        "                                       total=len(test_loader), desc='Validation')):\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # This gets the prediction from the network.\n",
        "            output = model(x)\n",
        "            output = output.squeeze(1)\n",
        "            # Sum up batch loss.\n",
        "            t_loss += criterion(output, y.float()).item()\n",
        "\n",
        "            # Get the prediction\n",
        "            pred = output\n",
        "\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "\n",
        "    t_loss /= len(test_loader)\n",
        "\n",
        "    precision_, recall_, thresholds = precision_recall_curve(targets, predictions)\n",
        "    f_measures = 2 * (precision_ * recall_) / (precision_ + recall_ + 0.0000000001)\n",
        "\n",
        "    # Select best threshold based on F2 score. Following previous works procedure.\n",
        "    ix_best = np.argmax(f_measures)\n",
        "    if ix_best > 0:\n",
        "        best_threshold = (thresholds[ix_best] + thresholds[ix_best - 1]) / 2\n",
        "    else:\n",
        "        best_threshold = thresholds[ix_best]\n",
        "    precision = precision_[ix_best]\n",
        "    recall = recall_[ix_best]\n",
        "\n",
        "    classifications = predictions > best_threshold\n",
        "\n",
        "    FPR, TPR, _ = roc_curve(targets, predictions)\n",
        "    AUC = auc(FPR, TPR)\n",
        "    AP = average_precision_score(targets, predictions)\n",
        "\n",
        "    # Calculate predictions based on best threshold.\n",
        "    correct = np.sum(classifications == targets)\n",
        "    accuracy = 100. * correct / len(classifications)\n",
        "\n",
        "    print('AVG loss: {:.4f}, ACC: {}/{} ({:.0f}%), Precision: {:.4f}, Recall: {:.4f}, AP: {:.4f}'.format(\n",
        "            t_loss, correct, len(test_loader.dataset), accuracy, precision, recall, AP))\n",
        "\n",
        "    # log metrics\n",
        "    log_dict['val_ACC'] = accuracy\n",
        "    log_dict['val_PRECISION'] = precision\n",
        "    log_dict['val_RECALL'] = recall\n",
        "    log_dict['val_AP'] = AP\n",
        "\n",
        "    return log_dict\n"
      ],
      "metadata": {
        "id": "dXjJLHr_lsPc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    # Set the seed for reproducibility.\n",
        "    torch.manual_seed(args.seed)\n",
        "    # Set the device.\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    add_augmented = args.add_augmented\n",
        "    num_augmented = args.num_augmented\n",
        "    zero_shot = args.zero_shot\n",
        "    logging = args.logging\n",
        "\n",
        "    run_name = f'KSDD2ResNet50-zero_shot_{zero_shot}-add_augmented_{add_augmented}-num_augmented_{num_augmented}-bs_{args.batch_size}-epochs_{args.epochs}'\n",
        "    tags = [f'{args.epochs}epochs', f'{num_augmented}augmented']\n",
        "    if args.zero_shot:\n",
        "        tags.append('zero_shot')\n",
        "    else:\n",
        "        tags.append('full_shot')\n",
        "    if args.add_augmented:\n",
        "        tags.append('augmented')\n",
        "    else:\n",
        "        tags.append('not_augmented')\n",
        "\n",
        "    if logging:\n",
        "        # Start a new wandb run to track this script.\n",
        "        wandb.init(\n",
        "            name=run_name,\n",
        "            config=args,\n",
        "            tags=tags\n",
        "        )\n",
        "\n",
        "    # Dataset.\n",
        "    print('Loading KolektorSDD2 training set...')\n",
        "    train_data = KolektorSDD2(dataroot=args.dataset_path, split='train', add_augmented=add_augmented, num_augmented=num_augmented, zero_shot=zero_shot)\n",
        "    print('Number of samples:', len(train_data))\n",
        "\n",
        "    print('Loading KolektorSDD2 test set...')\n",
        "    test_data = KolektorSDD2(dataroot=args.dataset_path, split='test')\n",
        "    print('Number of samples:', len(test_data))\n",
        "\n",
        "    # DataLoaders.\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size,\n",
        "                              shuffle=True, num_workers=args.num_workers)\n",
        "    test_loader = DataLoader(test_data, batch_size=args.batch_size,\n",
        "                             shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    # Define the model.\n",
        "    model = KSDD2ResNet50()\n",
        "    model.to(device)\n",
        "\n",
        "    # Define the loss function and the optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Training step.\n",
        "    print(f'Start training on {device} [...]')\n",
        "    model.train()\n",
        "    log_dict = {'train_loss': 0, 'val_ACC': 0, 'val_PRECISION': 0, 'val_RECALL': 0, 'val_AP': 0, 'epoch': 0}\n",
        "    for e in range(args.epochs):\n",
        "        epoch_loss = 0\n",
        "        for _, data in (tepoch := tqdm(enumerate(train_loader), unit='batch',\n",
        "                                       total=len(train_loader))):\n",
        "            tepoch.set_description(f'Epoch {e}')\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # Training step for the single batch.\n",
        "            model.zero_grad()\n",
        "            outputs = model(x)\n",
        "            outputs = outputs.squeeze(1)\n",
        "            loss = criterion(outputs, y.float())\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print statistics.\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "            if logging:\n",
        "                wandb.log({'train_loss':loss.item()})\n",
        "        epoch_loss /= len(train_loader)\n",
        "        log_dict['epoch_loss'] = epoch_loss\n",
        "        log_dict['epoch'] = e\n",
        "\n",
        "        # Evaluation step after each epoch.\n",
        "        eval_dict = evaluate(model, criterion, test_loader, device, log_dict)\n",
        "        if logging:\n",
        "            wandb.log(eval_dict)\n",
        "\n",
        "    if logging:\n",
        "        wandb.finish()\n",
        "    print('Training finished.')\n"
      ],
      "metadata": {
        "id": "jtjUankxaSr8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='DIAG training')\n",
        "    parser.add_argument('--seed', type=int, default=1234)\n",
        "    parser.add_argument('--epochs', type=int, default=30)\n",
        "    parser.add_argument('--batch_size', type=int, default=32)\n",
        "    parser.add_argument('--num_workers', type=int, default=8)\n",
        "    parser.add_argument('--dataset_path', type=str, required=True)\n",
        "    parser.add_argument('--add_augmented', action='store_true', help='Add augmented images to the training set')\n",
        "    parser.add_argument('--num_augmented', type=int, default=120)\n",
        "    parser.add_argument('--zero_shot', action='store_true', help='Use zero-shot learning mode')\n",
        "    parser.add_argument('--logging', action='store_true', help='Enable Weights & Biases logging')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "aYEM0J5Gl28_",
        "outputId": "e7e22535-120c-42ba-e70a-788b76b9f0db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--seed SEED] [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
            "                                [--num_workers NUM_WORKERS] --dataset_path DATASET_PATH\n",
            "                                [--add_augmented] [--num_augmented NUM_AUGMENTED] [--zero_shot]\n",
            "                                [--logging]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --dataset_path\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KSDD2ResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KSDD2ResNet50, self).__init__()\n",
        "        self.model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Instantiate and print the model to verify\n",
        "model = KSDD2ResNet50()\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BzsQC_VaoTL",
        "outputId": "c4451c0f-9182-4a65-9768-4712fcbd331b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 219MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KSDD2ResNet50(\n",
            "  (model): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (3): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (4): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (5): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): Bottleneck(\n",
            "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (2): Bottleneck(\n",
            "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data.ksdd2 import KolektorSDD2\n",
        "\n",
        "dataset_path = \"/content/ksdd2_preprocessed_1\"\n",
        "train_data = KolektorSDD2(dataroot=dataset_path, split='train', add_augmented=True, num_augmented=100, zero_shot=True)\n",
        "test_data = KolektorSDD2(dataroot=dataset_path, split='test')\n",
        "\n",
        "print('Number of training samples:', len(train_data))\n",
        "\n",
        "print('Number of testing samples:', len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIPxfHhkbLgm",
        "outputId": "861b00c4-451c-42d0-f943-9d288dc703c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 2185\n",
            "Number of testing samples: 1004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_workers = 8\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "5WwspbOXcFV1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = KSDD2ResNet50().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "model.train()\n",
        "for e in range(1):  # Run just for 1 epoch to start\n",
        "    epoch_loss = 0\n",
        "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader), desc='Training'):\n",
        "        x, y = data[0].to(device), data[1].to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(x)\n",
        "        outputs = outputs.squeeze(1)\n",
        "        loss = criterion(outputs, y.float())\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Batch {i+1}/{len(train_loader)}, Loss: {loss.item()}')\n",
        "    epoch_loss /= len(train_loader)\n",
        "    print(f'Epoch {e+1}, Avg Loss: {epoch_loss}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU9H9eQycMY8",
        "outputId": "bd6ee3f5-01eb-4a4a-905f-0f6dedeb5c9e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   3%|▎         | 2/69 [00:01<00:46,  1.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1/69, Loss: 0.6015614867210388\n",
            "Batch 2/69, Loss: 0.5497897863388062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   6%|▌         | 4/69 [00:01<00:21,  3.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 3/69, Loss: 0.4852888584136963\n",
            "Batch 4/69, Loss: 0.43772560358047485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   9%|▊         | 6/69 [00:02<00:14,  4.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 5/69, Loss: 0.3800646662712097\n",
            "Batch 6/69, Loss: 0.33996346592903137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  12%|█▏        | 8/69 [00:02<00:11,  5.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 7/69, Loss: 0.2937583029270172\n",
            "Batch 8/69, Loss: 0.24759641289710999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  14%|█▍        | 10/69 [00:02<00:10,  5.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 9/69, Loss: 0.21408003568649292\n",
            "Batch 10/69, Loss: 0.18430927395820618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  17%|█▋        | 12/69 [00:03<00:09,  6.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 11/69, Loss: 0.1565229892730713\n",
            "Batch 12/69, Loss: 0.2153564691543579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  20%|██        | 14/69 [00:03<00:08,  6.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 13/69, Loss: 0.09870131313800812\n",
            "Batch 14/69, Loss: 0.07382211089134216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  23%|██▎       | 16/69 [00:03<00:08,  6.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 15/69, Loss: 0.13236092031002045\n",
            "Batch 16/69, Loss: 0.044164564460515976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  26%|██▌       | 18/69 [00:04<00:07,  6.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 17/69, Loss: 0.09449302405118942\n",
            "Batch 18/69, Loss: 0.03770321235060692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  29%|██▉       | 20/69 [00:04<00:07,  6.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 19/69, Loss: 0.028406817466020584\n",
            "Batch 20/69, Loss: 0.08106152713298798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  32%|███▏      | 22/69 [00:04<00:07,  6.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 21/69, Loss: 0.033178191632032394\n",
            "Batch 22/69, Loss: 0.026765193790197372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  35%|███▍      | 24/69 [00:05<00:07,  6.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 23/69, Loss: 0.03609282150864601\n",
            "Batch 24/69, Loss: 0.02138052135705948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 26/69 [00:05<00:06,  6.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 25/69, Loss: 0.016015993431210518\n",
            "Batch 26/69, Loss: 0.17608287930488586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  41%|████      | 28/69 [00:05<00:06,  6.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 27/69, Loss: 0.01392324548214674\n",
            "Batch 28/69, Loss: 0.030712909996509552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  43%|████▎     | 30/69 [00:06<00:06,  6.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 29/69, Loss: 0.09192188084125519\n",
            "Batch 30/69, Loss: 0.04925891011953354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  46%|████▋     | 32/69 [00:06<00:05,  6.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 31/69, Loss: 0.013165553100407124\n",
            "Batch 32/69, Loss: 0.05836153402924538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  49%|████▉     | 34/69 [00:06<00:05,  6.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 33/69, Loss: 0.020230960100889206\n",
            "Batch 34/69, Loss: 0.011793842539191246\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  52%|█████▏    | 36/69 [00:06<00:05,  6.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 35/69, Loss: 0.05208897218108177\n",
            "Batch 36/69, Loss: 0.010344893671572208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  55%|█████▌    | 38/69 [00:07<00:04,  6.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 37/69, Loss: 0.012020508758723736\n",
            "Batch 38/69, Loss: 0.01955696940422058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  58%|█████▊    | 40/69 [00:07<00:04,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 39/69, Loss: 0.008591564372181892\n",
            "Batch 40/69, Loss: 0.02164541743695736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  61%|██████    | 42/69 [00:07<00:04,  6.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 41/69, Loss: 0.02219163253903389\n",
            "Batch 42/69, Loss: 0.10036249458789825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  64%|██████▍   | 44/69 [00:08<00:03,  6.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 43/69, Loss: 0.007099493406713009\n",
            "Batch 44/69, Loss: 0.07050681859254837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  67%|██████▋   | 46/69 [00:08<00:03,  6.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 45/69, Loss: 0.02570556104183197\n",
            "Batch 46/69, Loss: 0.01635439693927765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  70%|██████▉   | 48/69 [00:08<00:03,  6.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 47/69, Loss: 0.029557598754763603\n",
            "Batch 48/69, Loss: 0.007596891839057207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  72%|███████▏  | 50/69 [00:09<00:02,  6.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 49/69, Loss: 0.007897705771028996\n",
            "Batch 50/69, Loss: 0.04082096368074417\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  75%|███████▌  | 52/69 [00:09<00:02,  6.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 51/69, Loss: 0.007283741142600775\n",
            "Batch 52/69, Loss: 0.0425315760076046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  78%|███████▊  | 54/69 [00:09<00:02,  6.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 53/69, Loss: 0.007012955844402313\n",
            "Batch 54/69, Loss: 0.0038287811912596226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████  | 56/69 [00:10<00:02,  6.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 55/69, Loss: 0.02444934844970703\n",
            "Batch 56/69, Loss: 0.0048226723447442055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  84%|████████▍ | 58/69 [00:10<00:01,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 57/69, Loss: 0.011991560459136963\n",
            "Batch 58/69, Loss: 0.0072829946875572205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  87%|████████▋ | 60/69 [00:10<00:01,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 59/69, Loss: 0.006961638107895851\n",
            "Batch 60/69, Loss: 0.008935466408729553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  90%|████████▉ | 62/69 [00:11<00:01,  6.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 61/69, Loss: 0.014601469039916992\n",
            "Batch 62/69, Loss: 0.004522162489593029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  93%|█████████▎| 64/69 [00:11<00:00,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 63/69, Loss: 0.009325964376330376\n",
            "Batch 64/69, Loss: 0.007270270027220249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  96%|█████████▌| 66/69 [00:11<00:00,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 65/69, Loss: 0.004830939695239067\n",
            "Batch 66/69, Loss: 0.008705362677574158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  99%|█████████▊| 68/69 [00:12<00:00,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 67/69, Loss: 0.015971295535564423\n",
            "Batch 68/69, Loss: 0.0035599928814917803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining: 100%|██████████| 69/69 [00:12<00:00,  6.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 69/69, Loss: 0.01382078044116497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining: 100%|██████████| 69/69 [00:12<00:00,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Avg Loss: 0.08631400192709829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, criterion, test_loader, device):\n",
        "    t_loss = 0\n",
        "    targets = []\n",
        "    predictions = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(test_loader), total=len(test_loader), desc='Validation'):\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "            output = model(x)\n",
        "            output = output.squeeze(1)\n",
        "            t_loss += criterion(output, y.float()).item()\n",
        "            predictions.extend(output.cpu().numpy())\n",
        "            targets.extend(y.cpu().numpy())\n",
        "\n",
        "    print(f'Validation Loss: {t_loss / len(test_loader)}')\n",
        "    print('Evaluation done.')\n",
        "    return t_loss, predictions, targets\n",
        "\n",
        "# Call the evaluate function\n",
        "evaluate(model, criterion, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIt8QgWqccWL",
        "outputId": "d351091d-0b5e-432a-ba46-8672433dc2d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 32/32 [00:02<00:00, 10.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.52688232017681\n",
            "Evaluation done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16.86023424565792,\n",
              " [-5.6137075,\n",
              "  -5.8409142,\n",
              "  -5.8085938,\n",
              "  -4.13416,\n",
              "  -3.429883,\n",
              "  -5.7263656,\n",
              "  -2.8623405,\n",
              "  -5.9000998,\n",
              "  -4.2219124,\n",
              "  -4.490421,\n",
              "  -3.0350342,\n",
              "  -5.748627,\n",
              "  -6.307017,\n",
              "  -5.907183,\n",
              "  -4.9119678,\n",
              "  -4.814848,\n",
              "  -6.0526066,\n",
              "  -4.6519666,\n",
              "  -4.66563,\n",
              "  -3.421134,\n",
              "  -4.7465515,\n",
              "  -5.822961,\n",
              "  -4.020188,\n",
              "  -5.4318395,\n",
              "  -6.302316,\n",
              "  -5.426178,\n",
              "  -2.0299616,\n",
              "  -6.232738,\n",
              "  -3.329359,\n",
              "  -2.5681534,\n",
              "  -6.1141896,\n",
              "  -5.894373,\n",
              "  -5.7558618,\n",
              "  -2.8727932,\n",
              "  -4.7108774,\n",
              "  -6.2953343,\n",
              "  -4.4503675,\n",
              "  -5.147307,\n",
              "  -6.5961185,\n",
              "  -4.4409013,\n",
              "  -5.5917535,\n",
              "  -4.664763,\n",
              "  -5.4678874,\n",
              "  -5.719468,\n",
              "  -4.5620246,\n",
              "  -4.5641418,\n",
              "  -4.1385436,\n",
              "  -3.4052784,\n",
              "  -6.4087005,\n",
              "  -5.008062,\n",
              "  -4.1059365,\n",
              "  -4.7022614,\n",
              "  -4.014975,\n",
              "  -5.4892516,\n",
              "  -5.386291,\n",
              "  -6.197428,\n",
              "  -4.9739122,\n",
              "  -6.5939336,\n",
              "  -5.416896,\n",
              "  -4.2943354,\n",
              "  -4.9241424,\n",
              "  -3.2073557,\n",
              "  -5.765776,\n",
              "  -4.1415753,\n",
              "  -6.323658,\n",
              "  -4.2982984,\n",
              "  -4.759971,\n",
              "  -5.423672,\n",
              "  -4.8141923,\n",
              "  -6.4993653,\n",
              "  -6.164285,\n",
              "  -5.53694,\n",
              "  -4.436136,\n",
              "  -5.948984,\n",
              "  -5.773025,\n",
              "  -2.6378624,\n",
              "  -4.9542365,\n",
              "  -2.6959991,\n",
              "  -6.458073,\n",
              "  -5.478096,\n",
              "  -6.7368484,\n",
              "  -4.877872,\n",
              "  -4.211214,\n",
              "  -4.3383093,\n",
              "  -4.9787755,\n",
              "  -6.589876,\n",
              "  -2.9989293,\n",
              "  -4.0954456,\n",
              "  -5.4277806,\n",
              "  -5.2612243,\n",
              "  -5.5469174,\n",
              "  -4.2225738,\n",
              "  -6.0726414,\n",
              "  -4.983808,\n",
              "  -5.915231,\n",
              "  -4.7777734,\n",
              "  -5.4819026,\n",
              "  -5.7192917,\n",
              "  -5.2866025,\n",
              "  -4.7354856,\n",
              "  -5.58243,\n",
              "  -4.6174846,\n",
              "  -6.1977973,\n",
              "  -6.347339,\n",
              "  -4.2061,\n",
              "  -5.3330255,\n",
              "  -4.5376043,\n",
              "  -5.9904175,\n",
              "  -6.0770545,\n",
              "  -5.3932824,\n",
              "  -5.2601414,\n",
              "  -5.1592526,\n",
              "  -3.6253529,\n",
              "  -5.6977496,\n",
              "  -3.8769343,\n",
              "  -5.0832553,\n",
              "  -3.3944027,\n",
              "  -3.8765967,\n",
              "  -4.6831913,\n",
              "  -5.3813787,\n",
              "  -5.859756,\n",
              "  -6.6658826,\n",
              "  -5.3806005,\n",
              "  -4.807672,\n",
              "  -5.2272553,\n",
              "  -2.2611067,\n",
              "  -6.0210643,\n",
              "  -5.520292,\n",
              "  -2.0023391,\n",
              "  -4.626129,\n",
              "  -3.265778,\n",
              "  -5.159113,\n",
              "  -5.6490173,\n",
              "  -5.3927503,\n",
              "  -6.475393,\n",
              "  -5.251602,\n",
              "  -5.5368295,\n",
              "  -5.034668,\n",
              "  -3.8331852,\n",
              "  -2.5275614,\n",
              "  -5.3012733,\n",
              "  -5.9763227,\n",
              "  -5.238092,\n",
              "  -5.836866,\n",
              "  -2.2898421,\n",
              "  -5.5835395,\n",
              "  -5.9035716,\n",
              "  -3.3308837,\n",
              "  -5.457494,\n",
              "  -4.895328,\n",
              "  -6.6136637,\n",
              "  -4.14747,\n",
              "  -2.3684409,\n",
              "  -5.7257485,\n",
              "  -5.13535,\n",
              "  -3.7605674,\n",
              "  -5.459514,\n",
              "  -5.278801,\n",
              "  -5.976689,\n",
              "  -5.3477426,\n",
              "  -5.5983515,\n",
              "  -4.706472,\n",
              "  -4.5945625,\n",
              "  -4.9088745,\n",
              "  -4.2544565,\n",
              "  -5.796972,\n",
              "  -6.2368426,\n",
              "  -5.3098273,\n",
              "  -6.154426,\n",
              "  -4.509056,\n",
              "  -6.239683,\n",
              "  -2.5515323,\n",
              "  -5.160128,\n",
              "  -4.0677667,\n",
              "  -4.8086667,\n",
              "  -4.5981297,\n",
              "  -4.0999184,\n",
              "  -4.9649158,\n",
              "  -5.711182,\n",
              "  -6.253817,\n",
              "  -3.2065177,\n",
              "  -4.639645,\n",
              "  -5.2644,\n",
              "  -5.256269,\n",
              "  -2.5390406,\n",
              "  -6.1655426,\n",
              "  -2.4620063,\n",
              "  -6.304054,\n",
              "  -6.415982,\n",
              "  -5.318421,\n",
              "  -5.436154,\n",
              "  -5.6110644,\n",
              "  -5.5747643,\n",
              "  -3.805273,\n",
              "  -5.0654483,\n",
              "  -3.5090022,\n",
              "  -4.8153796,\n",
              "  -5.178842,\n",
              "  -5.875402,\n",
              "  -3.932098,\n",
              "  -3.3071234,\n",
              "  -4.1151595,\n",
              "  -5.1418753,\n",
              "  -5.466226,\n",
              "  -1.3720613,\n",
              "  -4.264563,\n",
              "  -5.983254,\n",
              "  -6.217936,\n",
              "  -5.2500234,\n",
              "  -5.441036,\n",
              "  -5.4961934,\n",
              "  -4.22553,\n",
              "  -5.821964,\n",
              "  -6.1110425,\n",
              "  -5.7326503,\n",
              "  -5.879375,\n",
              "  -3.5803185,\n",
              "  -6.5130744,\n",
              "  -5.2389154,\n",
              "  -4.621036,\n",
              "  -5.231698,\n",
              "  -6.0502257,\n",
              "  -4.1508574,\n",
              "  -2.9629235,\n",
              "  -6.332593,\n",
              "  -5.2440114,\n",
              "  -5.3965907,\n",
              "  -5.8304834,\n",
              "  -6.1813064,\n",
              "  -3.394436,\n",
              "  -5.554872,\n",
              "  -5.6577168,\n",
              "  -6.445172,\n",
              "  -3.8578224,\n",
              "  -6.347399,\n",
              "  -4.3891263,\n",
              "  -5.7914224,\n",
              "  -3.1926785,\n",
              "  -3.8607883,\n",
              "  -5.7833514,\n",
              "  -4.9712543,\n",
              "  -3.0065851,\n",
              "  -4.3738637,\n",
              "  -5.132296,\n",
              "  -4.9863515,\n",
              "  -5.935003,\n",
              "  -4.9353833,\n",
              "  -4.404788,\n",
              "  -5.5546584,\n",
              "  -4.7095237,\n",
              "  -4.7693896,\n",
              "  -1.780824,\n",
              "  -6.6801724,\n",
              "  -5.443521,\n",
              "  -5.4512053,\n",
              "  -5.9566083,\n",
              "  -4.8351264,\n",
              "  -6.0120254,\n",
              "  -4.46831,\n",
              "  -5.1364064,\n",
              "  -6.450913,\n",
              "  -2.310689,\n",
              "  -4.052444,\n",
              "  -5.452045,\n",
              "  -5.846473,\n",
              "  -5.572927,\n",
              "  -6.4980083,\n",
              "  -2.6433268,\n",
              "  -5.842748,\n",
              "  -5.7531567,\n",
              "  -4.8220787,\n",
              "  -5.031878,\n",
              "  -5.4316187,\n",
              "  -5.704088,\n",
              "  -5.0078773,\n",
              "  -5.8113704,\n",
              "  -5.33947,\n",
              "  -4.1098757,\n",
              "  -3.9691596,\n",
              "  -5.4259815,\n",
              "  -6.2473717,\n",
              "  -6.292953,\n",
              "  -2.4262455,\n",
              "  -4.8402333,\n",
              "  -5.6531854,\n",
              "  -3.6136575,\n",
              "  -4.7910066,\n",
              "  -4.3820786,\n",
              "  -5.963037,\n",
              "  -5.428061,\n",
              "  -3.0904598,\n",
              "  -5.0423913,\n",
              "  -5.6137238,\n",
              "  -6.23798,\n",
              "  -3.1399887,\n",
              "  -4.540306,\n",
              "  -5.1131225,\n",
              "  -6.0479417,\n",
              "  -6.3433347,\n",
              "  -4.454367,\n",
              "  -4.0407434,\n",
              "  -5.406036,\n",
              "  -5.0029616,\n",
              "  -4.9884048,\n",
              "  -5.0049148,\n",
              "  -5.104445,\n",
              "  -6.3807783,\n",
              "  -5.706075,\n",
              "  -5.5513234,\n",
              "  -4.1868663,\n",
              "  -5.566694,\n",
              "  -4.87418,\n",
              "  -5.374478,\n",
              "  -5.925038,\n",
              "  -4.3712163,\n",
              "  -5.8477793,\n",
              "  -5.7922273,\n",
              "  -3.8644285,\n",
              "  -5.7635894,\n",
              "  -5.5257263,\n",
              "  -2.1130471,\n",
              "  -4.0224032,\n",
              "  -3.1470342,\n",
              "  -6.076231,\n",
              "  -3.9673297,\n",
              "  -4.4761643,\n",
              "  -5.06788,\n",
              "  -4.9295835,\n",
              "  -4.3721614,\n",
              "  -5.7491775,\n",
              "  -5.763719,\n",
              "  -4.364125,\n",
              "  -5.6206174,\n",
              "  -6.0879207,\n",
              "  -3.3847895,\n",
              "  -5.6192613,\n",
              "  -5.037802,\n",
              "  -3.900756,\n",
              "  -5.086712,\n",
              "  -3.2959976,\n",
              "  -4.9835753,\n",
              "  -4.1248026,\n",
              "  -3.798515,\n",
              "  -4.088492,\n",
              "  -4.429,\n",
              "  -4.692265,\n",
              "  -5.5791297,\n",
              "  -3.6271424,\n",
              "  -6.4746137,\n",
              "  -5.8700852,\n",
              "  -4.642875,\n",
              "  -5.149979,\n",
              "  -5.9711695,\n",
              "  -4.66202,\n",
              "  -5.493317,\n",
              "  -5.424963,\n",
              "  -5.6240172,\n",
              "  -4.3712416,\n",
              "  -5.2884827,\n",
              "  -4.805363,\n",
              "  -5.3586164,\n",
              "  -3.8772984,\n",
              "  -4.851048,\n",
              "  -5.9322886,\n",
              "  -5.7057595,\n",
              "  -5.7120075,\n",
              "  -5.8296223,\n",
              "  -5.4130826,\n",
              "  -5.496894,\n",
              "  -3.7253397,\n",
              "  -2.416852,\n",
              "  -6.0627937,\n",
              "  -5.5721593,\n",
              "  -5.357742,\n",
              "  -6.242099,\n",
              "  -5.786964,\n",
              "  -4.1650906,\n",
              "  -5.012159,\n",
              "  -5.603564,\n",
              "  -5.6412573,\n",
              "  -5.6804175,\n",
              "  -3.8948889,\n",
              "  -5.463776,\n",
              "  -5.186172,\n",
              "  -4.3608804,\n",
              "  -5.244935,\n",
              "  -4.455586,\n",
              "  -5.2540507,\n",
              "  -5.943018,\n",
              "  -4.6172376,\n",
              "  -5.5574775,\n",
              "  -5.2079334,\n",
              "  -3.5566473,\n",
              "  -6.431053,\n",
              "  -6.7662826,\n",
              "  -5.662692,\n",
              "  -5.7032886,\n",
              "  -4.423177,\n",
              "  -5.8775725,\n",
              "  -6.4264383,\n",
              "  -3.6970778,\n",
              "  -4.1043715,\n",
              "  -5.3463564,\n",
              "  -5.498548,\n",
              "  -5.577232,\n",
              "  -4.775123,\n",
              "  -5.829262,\n",
              "  -2.3600707,\n",
              "  -5.402461,\n",
              "  -4.3450046,\n",
              "  -5.530025,\n",
              "  -3.0400224,\n",
              "  -5.375817,\n",
              "  -6.1398735,\n",
              "  -5.0511622,\n",
              "  -5.5475116,\n",
              "  -5.2718997,\n",
              "  -6.121286,\n",
              "  -5.4683795,\n",
              "  -6.083055,\n",
              "  -3.006013,\n",
              "  -3.5443816,\n",
              "  -3.9696696,\n",
              "  -4.9556813,\n",
              "  -5.4163275,\n",
              "  -5.3228865,\n",
              "  -2.2982516,\n",
              "  -4.5870256,\n",
              "  -5.85218,\n",
              "  -4.120131,\n",
              "  -6.5376735,\n",
              "  -6.083641,\n",
              "  -5.1807704,\n",
              "  -4.1437006,\n",
              "  -3.808757,\n",
              "  -5.267143,\n",
              "  -4.351701,\n",
              "  -4.6426353,\n",
              "  -6.229213,\n",
              "  -5.82217,\n",
              "  -2.4790487,\n",
              "  -5.187139,\n",
              "  -5.208633,\n",
              "  -6.016244,\n",
              "  -5.341148,\n",
              "  -4.802299,\n",
              "  -5.7347245,\n",
              "  -4.6998444,\n",
              "  -4.1266828,\n",
              "  -5.4028654,\n",
              "  -5.0912538,\n",
              "  -5.585157,\n",
              "  -6.2627597,\n",
              "  -5.3368955,\n",
              "  -3.5842123,\n",
              "  -4.538696,\n",
              "  -4.898333,\n",
              "  -5.9769115,\n",
              "  -4.958084,\n",
              "  -5.60198,\n",
              "  -5.477718,\n",
              "  -6.940746,\n",
              "  -5.124647,\n",
              "  -5.946866,\n",
              "  -4.784364,\n",
              "  -5.6413875,\n",
              "  -4.5976377,\n",
              "  -5.4335475,\n",
              "  -3.1304975,\n",
              "  -5.416495,\n",
              "  -4.3915224,\n",
              "  -5.3823524,\n",
              "  -5.5204864,\n",
              "  -4.7355137,\n",
              "  -4.327904,\n",
              "  -3.5373085,\n",
              "  -5.8743834,\n",
              "  -4.3370547,\n",
              "  -6.3792424,\n",
              "  -4.605699,\n",
              "  -4.236996,\n",
              "  -1.9868126,\n",
              "  -3.2608776,\n",
              "  -5.6590495,\n",
              "  -5.5413013,\n",
              "  -5.325827,\n",
              "  -5.4272046,\n",
              "  -4.830929,\n",
              "  -3.047667,\n",
              "  -5.185513,\n",
              "  -5.3616853,\n",
              "  -5.7252097,\n",
              "  -5.7253118,\n",
              "  -5.714972,\n",
              "  -5.8762913,\n",
              "  -4.819218,\n",
              "  -3.3999906,\n",
              "  -6.2314715,\n",
              "  -2.2330322,\n",
              "  -4.084163,\n",
              "  -5.449719,\n",
              "  -3.9908338,\n",
              "  -5.6688643,\n",
              "  -3.4174597,\n",
              "  -4.864693,\n",
              "  -4.239534,\n",
              "  -5.226866,\n",
              "  -3.6283503,\n",
              "  -6.489364,\n",
              "  -4.751591,\n",
              "  -5.38307,\n",
              "  -5.2286916,\n",
              "  -6.189135,\n",
              "  -6.067948,\n",
              "  -2.931271,\n",
              "  -3.4309812,\n",
              "  -4.573607,\n",
              "  -3.5786443,\n",
              "  -3.8701487,\n",
              "  -5.1085734,\n",
              "  -6.5124574,\n",
              "  -3.3042486,\n",
              "  -4.4387355,\n",
              "  -5.6136613,\n",
              "  -4.5555167,\n",
              "  -3.8362193,\n",
              "  -4.403053,\n",
              "  -4.843764,\n",
              "  -3.9677224,\n",
              "  -5.018894,\n",
              "  -3.4539523,\n",
              "  -2.9545891,\n",
              "  -4.40737,\n",
              "  -6.0517592,\n",
              "  -6.4212375,\n",
              "  -5.013662,\n",
              "  -6.1819916,\n",
              "  -3.182403,\n",
              "  -5.8962893,\n",
              "  -5.133908,\n",
              "  -5.343201,\n",
              "  -6.0418444,\n",
              "  -4.586303,\n",
              "  -3.9904037,\n",
              "  -5.582924,\n",
              "  -3.9103274,\n",
              "  -4.128807,\n",
              "  -5.6817565,\n",
              "  -4.910346,\n",
              "  -4.978309,\n",
              "  -5.3002963,\n",
              "  -2.8431726,\n",
              "  -5.306086,\n",
              "  -1.0831567,\n",
              "  -5.410988,\n",
              "  -5.930908,\n",
              "  -3.2485998,\n",
              "  -5.0848355,\n",
              "  -3.3060713,\n",
              "  -5.9210095,\n",
              "  -5.5964546,\n",
              "  -6.4302382,\n",
              "  -4.1761227,\n",
              "  -3.9926362,\n",
              "  -5.7159743,\n",
              "  -4.467698,\n",
              "  -6.3064094,\n",
              "  -2.6875052,\n",
              "  -5.4619484,\n",
              "  -4.7597785,\n",
              "  -2.3130565,\n",
              "  -4.928309,\n",
              "  -5.9586406,\n",
              "  -5.6987486,\n",
              "  -4.0539775,\n",
              "  -5.0528016,\n",
              "  -5.383011,\n",
              "  -5.7419553,\n",
              "  -4.717708,\n",
              "  -5.853893,\n",
              "  -3.999989,\n",
              "  -5.175743,\n",
              "  -3.2181349,\n",
              "  -5.2645397,\n",
              "  -4.330273,\n",
              "  -5.9684086,\n",
              "  -6.307418,\n",
              "  -5.116911,\n",
              "  -5.7391596,\n",
              "  -5.1919866,\n",
              "  -5.318703,\n",
              "  -5.117142,\n",
              "  -5.526392,\n",
              "  -5.139915,\n",
              "  -5.6818867,\n",
              "  -5.6500177,\n",
              "  -4.6808047,\n",
              "  -5.5997624,\n",
              "  -2.194751,\n",
              "  -5.4132466,\n",
              "  -4.589029,\n",
              "  -6.4807067,\n",
              "  -5.675928,\n",
              "  -4.606358,\n",
              "  -6.063708,\n",
              "  -4.964185,\n",
              "  -5.4096456,\n",
              "  -6.3269563,\n",
              "  -5.145631,\n",
              "  -6.0068307,\n",
              "  -4.0406747,\n",
              "  -5.080878,\n",
              "  -5.887492,\n",
              "  -4.3064346,\n",
              "  -5.6659584,\n",
              "  -4.8905597,\n",
              "  -5.8269362,\n",
              "  -5.1652455,\n",
              "  -6.006811,\n",
              "  -5.2409515,\n",
              "  -5.790879,\n",
              "  -5.3911467,\n",
              "  -3.5745502,\n",
              "  -3.9120212,\n",
              "  -4.539505,\n",
              "  -6.0231743,\n",
              "  -2.9334693,\n",
              "  -4.413825,\n",
              "  -6.029015,\n",
              "  -4.9555116,\n",
              "  -4.4801254,\n",
              "  -5.4522934,\n",
              "  -3.6003752,\n",
              "  -5.4211564,\n",
              "  -5.7441926,\n",
              "  -4.1471667,\n",
              "  -5.002938,\n",
              "  -3.434108,\n",
              "  -4.529665,\n",
              "  -6.050115,\n",
              "  -5.531424,\n",
              "  -6.08534,\n",
              "  -4.9362025,\n",
              "  -6.3181725,\n",
              "  -6.3753433,\n",
              "  -6.3959394,\n",
              "  -4.232415,\n",
              "  -5.2486897,\n",
              "  -5.3511558,\n",
              "  -5.81572,\n",
              "  -3.8044584,\n",
              "  -6.2381415,\n",
              "  -3.3645413,\n",
              "  -1.7446185,\n",
              "  -5.1728415,\n",
              "  -3.3640947,\n",
              "  -3.8849888,\n",
              "  -4.854178,\n",
              "  -4.2874007,\n",
              "  -4.0010834,\n",
              "  -5.7254057,\n",
              "  -5.365081,\n",
              "  -5.021027,\n",
              "  -6.463703,\n",
              "  -5.4795365,\n",
              "  -5.108898,\n",
              "  -5.4114327,\n",
              "  -5.227882,\n",
              "  -5.426943,\n",
              "  -4.776917,\n",
              "  -6.264725,\n",
              "  -4.89476,\n",
              "  -3.6037276,\n",
              "  -5.886766,\n",
              "  -4.188643,\n",
              "  -6.654306,\n",
              "  -5.4594293,\n",
              "  -4.108747,\n",
              "  -4.8240385,\n",
              "  -5.3007703,\n",
              "  -5.88634,\n",
              "  -6.4225965,\n",
              "  -3.1427617,\n",
              "  -4.974318,\n",
              "  -3.3431814,\n",
              "  -5.760567,\n",
              "  -3.9152427,\n",
              "  -6.516683,\n",
              "  -5.7255073,\n",
              "  -6.0151525,\n",
              "  -3.5044398,\n",
              "  -6.172197,\n",
              "  -4.3763285,\n",
              "  -6.5300884,\n",
              "  -5.7211957,\n",
              "  -4.1282096,\n",
              "  -5.1358314,\n",
              "  -6.151055,\n",
              "  -4.158113,\n",
              "  -4.3457026,\n",
              "  -6.2133784,\n",
              "  -4.805783,\n",
              "  -4.848036,\n",
              "  -5.6180916,\n",
              "  -5.162269,\n",
              "  -6.0557437,\n",
              "  -3.2936535,\n",
              "  -2.079982,\n",
              "  -5.2242494,\n",
              "  -4.1963463,\n",
              "  -3.5011106,\n",
              "  -5.406768,\n",
              "  -5.579637,\n",
              "  -3.7130153,\n",
              "  -4.847937,\n",
              "  -5.8455205,\n",
              "  -5.527409,\n",
              "  -5.319977,\n",
              "  -4.543757,\n",
              "  -3.727854,\n",
              "  -6.1674714,\n",
              "  -5.673215,\n",
              "  -4.4729753,\n",
              "  -3.9351394,\n",
              "  -5.3783913,\n",
              "  -3.9532757,\n",
              "  -4.9144464,\n",
              "  -5.4521923,\n",
              "  -4.3004365,\n",
              "  -5.183746,\n",
              "  -5.8052907,\n",
              "  -4.605819,\n",
              "  -4.952867,\n",
              "  -6.2160625,\n",
              "  -2.3006105,\n",
              "  -5.569256,\n",
              "  -4.840635,\n",
              "  -5.6273704,\n",
              "  -0.18175505,\n",
              "  0.73518634,\n",
              "  -4.7180624,\n",
              "  -5.608491,\n",
              "  -0.63556904,\n",
              "  -5.5792203,\n",
              "  -6.3339944,\n",
              "  -5.588613,\n",
              "  -5.7587442,\n",
              "  -4.435638,\n",
              "  -5.011256,\n",
              "  -4.1752605,\n",
              "  -3.789339,\n",
              "  -3.8919322,\n",
              "  -2.504429,\n",
              "  -3.982498,\n",
              "  -5.946881,\n",
              "  -5.2496943,\n",
              "  -4.9840617,\n",
              "  -3.3816702,\n",
              "  -6.170255,\n",
              "  -4.9540496,\n",
              "  -5.8000517,\n",
              "  -4.3909974,\n",
              "  -2.8456113,\n",
              "  -5.746461,\n",
              "  -5.0911994,\n",
              "  -5.8847504,\n",
              "  -4.412661,\n",
              "  -5.047393,\n",
              "  -4.414284,\n",
              "  -6.251634,\n",
              "  -5.6475205,\n",
              "  -6.0405803,\n",
              "  -5.0124345,\n",
              "  -4.3373218,\n",
              "  -4.437388,\n",
              "  -5.1941423,\n",
              "  -5.538275,\n",
              "  -4.552897,\n",
              "  -5.89717,\n",
              "  -5.998263,\n",
              "  -4.569187,\n",
              "  -6.437048,\n",
              "  -5.633373,\n",
              "  -4.5967994,\n",
              "  -5.425462,\n",
              "  -5.1823797,\n",
              "  -4.4670777,\n",
              "  -4.931915,\n",
              "  -5.1133695,\n",
              "  -4.154724,\n",
              "  -5.215831,\n",
              "  -6.3139296,\n",
              "  -5.8052096,\n",
              "  -5.8368106,\n",
              "  -5.348887,\n",
              "  -4.8278275,\n",
              "  -2.783977,\n",
              "  -4.2713633,\n",
              "  -4.29889,\n",
              "  -4.080839,\n",
              "  -5.502436,\n",
              "  -5.6349845,\n",
              "  -5.0624595,\n",
              "  -5.2875743,\n",
              "  -5.136941,\n",
              "  -5.6315656,\n",
              "  -6.6026225,\n",
              "  -5.259174,\n",
              "  -6.2110825,\n",
              "  -2.8495955,\n",
              "  -6.202447,\n",
              "  -5.146966,\n",
              "  -5.0343776,\n",
              "  -5.7749524,\n",
              "  -5.602932,\n",
              "  -2.0664253,\n",
              "  -4.572246,\n",
              "  -4.6320605,\n",
              "  -4.401492,\n",
              "  -5.753389,\n",
              "  -3.62591,\n",
              "  -3.724558,\n",
              "  -4.99365,\n",
              "  -5.297815,\n",
              "  -4.545785,\n",
              "  -4.210082,\n",
              "  -6.260561,\n",
              "  -4.993595,\n",
              "  -4.1383624,\n",
              "  -4.3427005,\n",
              "  -6.1024995,\n",
              "  -5.007067,\n",
              "  -5.9872813,\n",
              "  -5.6320176,\n",
              "  -6.2167974,\n",
              "  -4.8303256,\n",
              "  -4.6378527,\n",
              "  -5.59602,\n",
              "  -5.9824986,\n",
              "  -6.4593463,\n",
              "  -5.678388,\n",
              "  -5.3755956,\n",
              "  -6.472428,\n",
              "  -6.007181,\n",
              "  -3.2385879,\n",
              "  -5.561637,\n",
              "  -4.929993,\n",
              "  -3.7903976,\n",
              "  -4.448014,\n",
              "  -5.9474835,\n",
              "  -5.3535953,\n",
              "  -5.1551156,\n",
              "  -4.3953943,\n",
              "  -5.772921,\n",
              "  -5.709084,\n",
              "  -6.592046,\n",
              "  -4.4725966,\n",
              "  -6.063157,\n",
              "  -6.046732,\n",
              "  -4.834155,\n",
              "  -5.6404643,\n",
              "  -5.2574334,\n",
              "  -4.607469,\n",
              "  -4.754282,\n",
              "  -2.1721306,\n",
              "  -5.084537,\n",
              "  -3.4478562,\n",
              "  -6.2966313,\n",
              "  -4.7441325,\n",
              "  -5.462199,\n",
              "  -6.5406575,\n",
              "  -5.630016,\n",
              "  -6.5519967,\n",
              "  -3.2776074,\n",
              "  -3.8569617,\n",
              "  -5.081884,\n",
              "  -3.000099,\n",
              "  -5.2881045,\n",
              "  -4.9310565,\n",
              "  -1.3155825,\n",
              "  -5.9900556,\n",
              "  -5.73279,\n",
              "  -5.2208576,\n",
              "  -4.7102027,\n",
              "  -6.1263137,\n",
              "  -5.155112,\n",
              "  -5.0334954,\n",
              "  -4.1426015,\n",
              "  -5.05343,\n",
              "  -3.5676272,\n",
              "  -5.139814,\n",
              "  -1.9320995,\n",
              "  -5.486701,\n",
              "  -3.951781,\n",
              "  -4.799751,\n",
              "  -5.677647,\n",
              "  -3.3679824,\n",
              "  -4.1842046,\n",
              "  -4.938232,\n",
              "  -5.646868,\n",
              "  -5.6243033,\n",
              "  -4.1542215,\n",
              "  -4.2903066,\n",
              "  -5.922175,\n",
              "  -4.555775,\n",
              "  -4.7689657,\n",
              "  -4.042296,\n",
              "  -5.2870183,\n",
              "  -5.4087853,\n",
              "  -3.5087729,\n",
              "  -6.189724,\n",
              "  -3.6762376,\n",
              "  -5.5677943,\n",
              "  -4.9717693,\n",
              "  -6.6876426,\n",
              "  -6.5981975,\n",
              "  -5.8913484,\n",
              "  -5.487984,\n",
              "  -5.2890844,\n",
              "  -3.4656615,\n",
              "  -4.926462,\n",
              "  -4.6128354,\n",
              "  -6.317616,\n",
              "  -3.5224617,\n",
              "  -4.773278,\n",
              "  -4.574815,\n",
              "  -4.110766,\n",
              "  -4.8799763,\n",
              "  -5.942163,\n",
              "  -5.3655686,\n",
              "  -4.4044337,\n",
              "  -4.8898716,\n",
              "  -3.7524352,\n",
              "  -5.8276925,\n",
              "  -4.412133,\n",
              "  -3.1081963,\n",
              "  -4.013583,\n",
              "  -5.936794,\n",
              "  -4.945393,\n",
              "  -4.1552606,\n",
              "  -4.8107214,\n",
              "  -7.624668,\n",
              "  -5.933226,\n",
              "  -5.812385,\n",
              "  -6.0019135,\n",
              "  -5.1371455,\n",
              "  -5.042812,\n",
              "  -5.0578947,\n",
              "  -5.56929,\n",
              "  -5.5449023,\n",
              "  -5.1702876,\n",
              "  -4.53001,\n",
              "  -6.094069,\n",
              "  -5.59336,\n",
              "  0.023451716,\n",
              "  -4.8224235,\n",
              "  -4.4253483,\n",
              "  -5.5638165,\n",
              "  -5.591785,\n",
              "  -6.3702974,\n",
              "  -5.9293523,\n",
              "  -5.9418325,\n",
              "  -4.9898024,\n",
              "  -2.319578,\n",
              "  -4.3538904,\n",
              "  -3.2708383,\n",
              "  -3.6123095,\n",
              "  -4.878582,\n",
              "  -5.634215,\n",
              "  -5.5876465,\n",
              "  -5.136405,\n",
              "  -5.3051085,\n",
              "  -5.9023085,\n",
              "  -5.581119,\n",
              "  -5.729029,\n",
              "  -3.8534663,\n",
              "  -6.122241,\n",
              "  -7.0046606,\n",
              "  -5.197353,\n",
              "  -6.7097726,\n",
              "  -6.0216794,\n",
              "  -4.7082534,\n",
              "  -4.293258,\n",
              "  -4.642669,\n",
              "  -4.9063964,\n",
              "  -3.4162807,\n",
              "  -4.84516,\n",
              "  -3.5464177,\n",
              "  -4.623981,\n",
              "  -5.1245985,\n",
              "  -5.3579817,\n",
              "  -3.1868854,\n",
              "  -2.6460648,\n",
              "  -4.9328256,\n",
              "  -3.8857427,\n",
              "  -3.9856849,\n",
              "  -5.991109,\n",
              "  -4.9368663,\n",
              "  -2.6767259,\n",
              "  -3.3098776,\n",
              "  ...],\n",
              " [0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  ...])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"KSDD2ResNet50\", config={\"epochs\": 30, \"batch_size\": 32})\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "# Log some example metrics\n",
        "wandb.log({\"example_metric\": 0.9})\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394,
          "referenced_widgets": [
            "9a3d7c16782c4cf497cf79e3c9d9b223",
            "eeca1277d61e4ad7acce9592255f2392",
            "a748413390b0458a87b1e60121cc42ca",
            "665d5a43613448bc87beba30cece4230",
            "173887c32400401faaebf51fed4bf9d8",
            "b2490c5310ef41a8b135b7ce800ab512",
            "21453f13cbbb425db521012e07c3bba8",
            "59583e9169244686adc523b1f2fb2842"
          ]
        },
        "id": "dACvGgM0ckM3",
        "outputId": "b8759908-40cc-4e2e-d605-f5cec8b83690"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/DIAG/wandb/run-20240902_063703-5man8upa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50/runs/5man8upa' target=\"_blank\">icy-silence-1</a></strong> to <a href='https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50/runs/5man8upa' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50/runs/5man8upa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.18781725888324874, max=1.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a3d7c16782c4cf497cf79e3c9d9b223"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>example_metric</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>example_metric</td><td>0.9</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">icy-silence-1</strong> at: <a href='https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50/runs/5man8upa' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50/runs/5man8upa</a><br/> View project at: <a href='https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/KSDD2ResNet50</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240902_063703-5man8upa/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    torch.manual_seed(args.seed)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    add_augmented = args.add_augmented\n",
        "    num_augmented = args.num_augmented\n",
        "    zero_shot = args.zero_shot\n",
        "    logging = args.logging\n",
        "\n",
        "    run_name = f'KSDD2ResNet50-zero_shot_{zero_shot}-add_augmented_{add_augmented}-num_augmented_{num_augmented}-bs_{args.batch_size}-epochs_{args.epochs}'\n",
        "    tags = [f'{args.epochs}epochs', f'{num_augmented}augmented']\n",
        "    tags.append('zero_shot' if args.zero_shot else 'full_shot')\n",
        "    tags.append('augmented' if args.add_augmented else 'not_augmented')\n",
        "\n",
        "    if logging:\n",
        "        wandb.init(name=run_name, config=args, tags=tags)\n",
        "\n",
        "    print('Loading KolektorSDD2 training set...')\n",
        "    train_data = KolektorSDD2(dataroot=args.dataset_path, split='train', add_augmented=add_augmented, num_augmented=num_augmented, zero_shot=zero_shot)\n",
        "    print('Number of samples:', len(train_data))\n",
        "\n",
        "    print('Loading KolektorSDD2 test set...')\n",
        "    test_data = KolektorSDD2(dataroot=args.dataset_path, split='test')\n",
        "    print('Number of samples:', len(test_data))\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
        "    test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    model = KSDD2ResNet50().to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    print(f'Start training on {device} [...]')\n",
        "    model.train()\n",
        "    log_dict = {'train_loss': 0, 'val_ACC': 0, 'val_PRECISION': 0, 'val_RECALL': 0, 'val_AP': 0, 'epoch': 0}\n",
        "    for e in range(args.epochs):\n",
        "        epoch_loss = 0\n",
        "        for _, data in (tepoch := tqdm(enumerate(train_loader), unit='batch', total=len(train_loader))):\n",
        "            tepoch.set_description(f'Epoch {e}')\n",
        "            x, y = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "            outputs = model(x).squeeze(1)\n",
        "            loss = criterion(outputs, y.float())\n",
        "            epoch_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "            if logging:\n",
        "                wandb.log({'train_loss': loss.item()})\n",
        "        epoch_loss /= len(train_loader)\n",
        "        log_dict['epoch_loss'] = epoch_loss\n",
        "        log_dict['epoch'] = e\n",
        "\n",
        "        eval_dict = evaluate(model, criterion, test_loader, device, log_dict)\n",
        "        if logging:\n",
        "            wandb.log(eval_dict)\n",
        "\n",
        "    if logging:\n",
        "        wandb.finish()\n",
        "    print('Training finished.')\n"
      ],
      "metadata": {
        "id": "hBg1N24LdIes"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self, seed, epochs, batch_size, num_workers, dataset_path, add_augmented, num_augmented, zero_shot, logging):\n",
        "        self.seed = seed\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.dataset_path = dataset_path\n",
        "        self.add_augmented = add_augmented\n",
        "        self.num_augmented = num_augmented\n",
        "        self.zero_shot = zero_shot\n",
        "        self.logging = logging\n",
        "\n",
        "args = Args(\n",
        "    seed=0,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    dataset_path='/content/ksdd2_preprocessed_1',\n",
        "    add_augmented=True,\n",
        "    num_augmented=100,\n",
        "    zero_shot=True,\n",
        "    logging=True\n",
        ")\n",
        "\n",
        "main(args)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "0_AaxxqsjD8l",
        "outputId": "5fe410dd-e4de-44c1-9eb8-7517de2bbcbb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhakankarasu97\u001b[0m (\u001b[33mhakankarasu97-kist\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/DIAG/wandb/run-20240902_070407-8ud64cyl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hakankarasu97-kist/DIAG/runs/8ud64cyl' target=\"_blank\">KSDD2ResNet50-zero_shot_True-add_augmented_True-num_augmented_100-bs_32-epochs_30</a></strong> to <a href='https://wandb.ai/hakankarasu97-kist/DIAG' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hakankarasu97-kist/DIAG' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/DIAG</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hakankarasu97-kist/DIAG/runs/8ud64cyl' target=\"_blank\">https://wandb.ai/hakankarasu97-kist/DIAG/runs/8ud64cyl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading KolektorSDD2 training set...\n",
            "Number of samples: 2185\n",
            "Loading KolektorSDD2 test set...\n",
            "Number of samples: 1004\n",
            "Start training on cuda [...]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 69/69 [00:12<00:00,  5.59batch/s, loss=0.0136]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "evaluate() takes 4 positional arguments but 5 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-1fea3cf3128c>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-243e8ad8dc47>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Evaluation step after each epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0meval_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: evaluate() takes 4 positional arguments but 5 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q6-Va5JXoQ7",
        "outputId": "9da2358b-322c-4b88-d43e-ebf3ac04971f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.13.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.13.0-py2.py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.13.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.8\n"
          ]
        }
      ]
    }
  ]
}